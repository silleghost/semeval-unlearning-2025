import os
import sys
import json
import glob
import math
import torch
import random
import shutil
import argparse
import datasets
import numpy as np
import pandas as pd

from tqdm import tqdm
from pathlib import Path
from accelerate import Accelerator
from collections import defaultdict
from statistics import mean, harmonic_mean
from rouge_score import rouge_scorer
from torch.utils.data import DataLoader
from sklearn.metrics import roc_curve, auc
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig

def get_args_and_verify():
    parser = argparse.ArgumentParser(description="Script to run inference and evaluation")
    parser.add_argument('--data_path', help="Path to unlearning dataset containing jsonl files")
    parser.add_argument('--checkpoint_path', help="Path to model checkpoint")
    parser.add_argument('--output_dir', required=False, default=None, help="Path to store inference files and evaluation results")
    parser.add_argument('--mia_data_path', required=False, default=None, help="Path to member and nonmember jsonl files for MIA attack")
    parser.add_argument('--mmlu_metrics_file_path', required=False, default=None, help="Path to metrics.json file generated by MMLU")
    parser.add_argument('--max_new_tokens', required=False, type=int, default=256, help='Maximum number of tokens to generate')
    parser.add_argument('--batch_size', required=False, type=int, default=25, help='Batch size for inference')
    parser.add_argument('--debug', required=False, default=False, action='store_true', help='Print detailed messages')
    parser.add_argument('--compute_metrics_only', required=False, default=False, action='store_true', help='Skip inference and compute metrics from inference files')
    parser.add_argument('--seed', required=False, default=42, help='Random seed for experiments')
    parser.add_argument('--keep_files', required=False, default=False, action='store_true', help='Retain intermediate files')
    args = parser.parse_args()

    if args.compute_metrics_only:
        args.keep_files = True

    if args.output_dir is None:
        args.output_dir = os.getcwd()
    else:
        args.output_dir = args.output_dir.rstrip('/')
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    # Verify data files exist
    assert(os.path.exists(args.data_path))
    assert(os.path.exists(os.path.join(args.data_path, 'forget.jsonl')))
    assert(os.path.exists(os.path.join(args.data_path, 'retain.jsonl')))

    # If specified, verify data files exist
    if args.mia_data_path is not None:
        assert(os.path.exists(args.mia_data_path))
        assert(os.path.exists(os.path.join(args.mia_data_path, 'member.jsonl')))
        assert(os.path.exists(os.path.join(args.mia_data_path, 'nonmember.jsonl')))
    else: # Add warning if MIA path not provided
        print("WARNING: MIA data path not provided, your final evaluation metric includes MIA test set performance so please rerun with this option to get the accurate performance. Proceeding for now")
        
    if args.mmlu_metrics_file_path is not None:
        assert(os.path.exists(args.mmlu_metrics_file_path))
    else: # Add warning if MMLU file not provided
        print("WARNING: MMLU metrics file not provided, your final evaluation metric includes MMLU aggregate performance so please run this test to get the accurate performance. Proceeding for now")

    return args

def inference(args, model, tokenizer):
    forget_file = args.data_path + 'forget.jsonl'
    retain_file = args.data_path + 'retain.jsonl'

    accelerator = Accelerator()
    model.to(accelerator.device)

    for split, train_file in [('retain', retain_file), ('forget', forget_file)]:
        data_files = {}
        dataset_args = {}
        if train_file is not None:
            data_files["train"] = train_file
        raw_datasets = datasets.load_dataset(
            "json",
            data_files=data_files,
            **dataset_args,
        )
        train_dataset = raw_datasets["train"]

        output_dic = defaultdict(lambda :{'id': [], 'task': [], 'input': [], 'expected_output': [], 'model_output': [], 'nll': []})

        with accelerator.split_between_processes(train_dataset, apply_padding=True) as data:
            for idx in tqdm(range(len(data['input']))):
                question, answer = data["input"][idx], data["output"][idx]
                output_dic[accelerator.process_index]['id'].append(data["id"][idx])
                output_dic[accelerator.process_index]['task'].append(data["task"][idx])
                output_dic[accelerator.process_index]['input'].append(data["input"][idx])
                output_dic[accelerator.process_index]['expected_output'].append(data["output"][idx])
                input_ids = tokenizer(
                    question,
                    return_tensors='pt'
                ).input_ids.to(model.device)

                combined_input_ids = tokenizer(
                    question+answer,
                    return_tensors='pt'
                ).input_ids.to(model.device)
                combined_target_ids = combined_input_ids.clone()
                combined_target_ids[:,:len(input_ids[0])] = -100

                with torch.no_grad():
                    out = model.generate(input_ids, max_new_tokens=args.max_new_tokens, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)
                    output_ids = out[:, len(input_ids[0]):]
                    output = tokenizer.batch_decode(
                        output_ids,
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=True)[0]
                    output_dic[accelerator.process_index]['model_output'].append(output)

                    # For Perplexity
                    out = model(combined_input_ids, labels=combined_target_ids)
                    if args.debug:
                        print(tokenizer.batch_decode(
                            torch.argmax(
                                torch.nn.functional.softmax(
                                    torch.tensor(out.logits),
                                    dim=2),
                                dim=2)[:, len(input_ids[0]):],
                            skip_special_tokens=True,
                            clean_up_tokenization_spaces=True)[0])
                    neg_log_likelihood = out.loss.item()
                    output_dic[accelerator.process_index]['nll'].append(neg_log_likelihood)

        accelerator.wait_for_everyone()
        
        if args.debug:
            print([len(value) for value in output_dic[accelerator.process_index].values()])
        output_df = pd.DataFrame.from_dict(output_dic[accelerator.process_index])
        
        output_file_name = f"{args.output_dir}/{split}_{accelerator.process_index}.csv"
        if args.debug:
            print('Saving to: ', output_file_name)
        output_df.to_csv(output_file_name, index=False)

def mia_attacks(args, model, tokenizer):
    member_file = args.mia_data_path + 'member.jsonl'
    nonmember_file = args.mia_data_path + 'nonmember.jsonl'

    accelerator = Accelerator()
    model.to(accelerator.device)

    for dataset, train_file in [('member', member_file), ('nonmember', nonmember_file)]:
        data_files = {}
        dataset_args = {}
        if train_file is not None:
            data_files["train"] = train_file
        raw_datasets = datasets.load_dataset(
            "json",
            data_files=data_files,
            **dataset_args,
        )
        train_dataset = raw_datasets["train"]

        output_dic = defaultdict(lambda :{'id': [], 'nll': []})

        with accelerator.split_between_processes(train_dataset, apply_padding=True) as data:
            for idx in tqdm(range(len(data['document']))):
                document = data["document"][idx]
                output_dic[accelerator.process_index]['id'].append(data["id"][idx])
                input_ids = tokenizer(
                    document,
                    return_tensors='pt'
                ).input_ids.to(model.device)

                target_ids = input_ids.clone()

                with torch.no_grad():
                    out = model(input_ids, labels=target_ids)
                    neg_log_likelihood = out.loss.item()
                    output_dic[accelerator.process_index]['nll'].append(neg_log_likelihood)

        accelerator.wait_for_everyone()
        
        output_df = pd.DataFrame.from_dict(output_dic[accelerator.process_index])
        
        results_dir = os.path.join(args.output_dir, 'mia_results')
        Path(results_dir).mkdir(parents=True, exist_ok=True)
        output_file_name = f"{results_dir}/{dataset}_{accelerator.process_index}.csv"
        if args.debug:
            print('Saving to: ', output_file_name)
        output_df.to_csv(output_file_name, index=False)

def compute_auc(member_loss, nonmember_loss):
    assert not np.any(np.isnan(member_loss))
    assert not np.any(np.isnan(nonmember_loss))
    combined_loss = member_loss + nonmember_loss 
    combined_loss = -1 * np.array(combined_loss)
    combined_labels = len(member_loss) * [1] + len(nonmember_loss) * [0]
    fp, tp, _ = roc_curve(combined_labels, combined_loss)

    auc_score = float(auc(fp, tp))

    return auc_score

def compute_metrics(args):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)

    results = {}
    aggregate_scores_list = []
    for split in ['forget', 'retain']:
        files = glob.glob(args.output_dir + '/{}_*.csv'.format(split))
        if len(files) == 0:
            print("[ERROR] Missing inference files, rerun script with inference first")
            return  # sys.exit(1) throws a long traceback so just return for now
        df_list = [pd.read_csv(f) for f in files]
        if not args.keep_files:
            _ = [os.remove(f) for f in files]
        df = pd.concat(df_list, ignore_index=True)

        df['regurgitation-score-rouge-1'] = None
        df['regurgitation-score'] = None
        df['knowledge-score'] = None
        ground_truths = df['expected_output'].tolist()
        gen_outputs = df['model_output'].tolist()

        for i, (gen, gt) in enumerate(zip(gen_outputs, ground_truths)):
            if df.loc[i, 'id'][:-1].endswith('sc'):
                rouge_scores = scorer.score(str(gt), str(gen))
                df.loc[i, 'regurgitation-score-rouge-1'] = rouge_scores['rouge1'].recall
                df.loc[i, 'regurgitation-score'] = rouge_scores['rougeL'].recall
            elif df.loc[i, 'id'][:-1].endswith('qa'):
                 df.loc[i, 'knowledge-score'] = int(str(gt).strip().lower() == str(gen).strip().lower())

        results[split+'-set'] = {'overall-regurgitation-score': np.mean(df['regurgitation-score']), 'overall-knowledge-score': np.mean(df['knowledge-score'])}
        split_aggregate_scores_dict = df.groupby('task')[['regurgitation-score', 'knowledge-score']].mean().to_dict(orient='index')
        results[split+'-set'].update(split_aggregate_scores_dict)
        split_aggregate_score_values = [float(val) for inner in split_aggregate_scores_dict.values() for val in inner.values()]
        if split == 'forget':
            split_aggregate_score_values = [(1 - val) for val in split_aggregate_score_values]

        aggregate_scores_list.extend(split_aggregate_score_values)

    if args.mia_data_path is not None:
        mia_results_dir = os.path.join(args.output_dir, 'mia_results')
        mia_results = {}
        for dataset in ['member', 'nonmember']:
            files = glob.glob(mia_results_dir + '/{}_*.csv'.format(dataset))
            if len(files) == 0:
                print("[ERROR] Missing mia files, rerun script with inference first")
                return  # sys.exit(1) throws a long traceback so just return for no
            df_list = [pd.read_csv(f) for f in files]
            df = pd.concat(df_list, ignore_index=True)
            mia_results[dataset] = df['nll'].tolist()
        
        if not args.keep_files:
            shutil.rmtree(mia_results_dir)

        auc = compute_auc(mia_results['member'], mia_results['nonmember'])
        # Best MIA rates we can get are ~0.5. 
        # Scores close to 1 suggest under-unlearning
        # Scores close to 0 suggest over-unlearning
        results['mia_loss_acc'] = auc
#        aggregate_scores_list.append(1 - auc) 

    if args.mmlu_metrics_file_path is not None:
        with open(args.mmlu_metrics_file_path) as inptr:
            mmlu_scores = json.loads(inptr.read())
        results['mmlu_average'] = mmlu_scores['average_acc']
#        aggregate_scores_list.append(mmlu_scores['average_acc'])
    
    results['aggregated-terms'] = aggregate_scores_list

    task_aggregate = harmonic_mean(aggregate_scores_list)
    results['aggregate-score'] = -1

    results['harmonic-mean-task-aggregate'] = task_aggregate

    # Need MMLU and MIA scores to compute the aggregate
    if 'mmlu_average' in results and 'mia_loss_acc' in results:
        if results['mmlu_average'] < 0.371:
            # MMLU score should not drop below 75% of pre-unlearning preformance
            print(f"[WARNING] The MMLU average for the provided checkpoint is below threshold. If this happens your model may not be considered in final challenge ranking.")

        mia_final_score = 1 - abs(results['mia_loss_acc'] - 0.5)*2
        results['mia_final_score'] = mia_final_score
        results['aggregate-score'] = mean([task_aggregate, results['mmlu_average'], mia_final_score])

    metrics_file = os.path.join(args.output_dir, 'evaluation_results.jsonl')
    with open(metrics_file, 'w') as outptr:
        outptr.write(json.dumps(results))

def main():
    args = get_args_and_verify()

    # Set random seed
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    if args.debug:
        print('Evaluating Checkpoint at {}'.format(args.checkpoint_path))

    checkpoint_path = args.checkpoint_path

    # Set up accelerator
    accelerator = Accelerator()
    if not args.compute_metrics_only:
        model = AutoModelForCausalLM.from_pretrained(checkpoint_path, torch_dtype=torch.bfloat16, trust_remote_code = True) # .to('cuda')

        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        tokenizer.pad_token = tokenizer.eos_token
    
        inference(args, model, tokenizer)

        if args.mia_data_path is not None:
            mia_attacks(args, model, tokenizer)

    if accelerator.is_main_process:
        compute_metrics(args)

if __name__ == '__main__':
    main()
